{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CcROQcVVoUiH"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jeffheaton/t81_558_deep_learning/blob/master/t81_558_class_08_4_bayesian_hyperparameter_opt.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Architecting Network: Hyperparameters\n",
        "\n",
        "The number of layers, neuron counts per layer, layer types, and activation functions are all choices we must make to optimize your neural network. Some of the categories of hyperparameters for you to choose from coming from the following list:\n",
        "\n",
        "* Number of Hidden Layers and Neuron Counts\n",
        "* Activation Functions\n",
        "* Advanced Activation Functions\n",
        "* Regularization: L1, L2, Dropout\n",
        "* Batch Normalization\n",
        "* Training Parameters\n",
        "\n",
        "## Number of Hidden Layers and Neuron Counts\n",
        "\n",
        "* **Activation** - You can also add activation functions as layers.  Using the activation layer is the same as specifying the activation function as part of a Dense (or other) layer type.\n",
        "* **ActivityRegularization** Used to add L1/L2 regularization outside of a layer. You can specify L1 and L2 as part of a Dense (or other) layer type.\n",
        "* **Dense** - The original neural network layer type. In this layer type, every neuron connects to the next layer. The input vector is one-dimensional, and placing specific inputs next does not affect each other. \n",
        "* **Dropout** - Dropout consists of randomly setting a fraction rate of input units to 0 at each update during training time, which helps prevent overfitting. Dropout only occurs during training.\n",
        "* **Flatten** - Flattens the input to 1D and does not affect the batch size.\n",
        "* **Input** - A Keras tensor is a tensor object from the underlying back end (Theano, TensorFlow, or CNTK), which we augment with specific attributes to build a Keras by knowing the inputs and outputs of the model.\n",
        "* **Lambda** - Wraps arbitrary expression as a Layer object.\n",
        "* **Masking** - Masks a sequence using a mask value to skip timesteps.\n",
        "* **Permute** - Permutes the input dimensions according to a given pattern. Useful for tasks such as connecting RNNs and convolutional networks.\n",
        "* **RepeatVector** - Repeats the input n times.\n",
        "* **Reshape** - Similar to Numpy reshapes.\n",
        "* **SpatialDropout1D** - This version performs the same function as Dropout; however, it drops entire 1D feature maps instead of individual elements. \n",
        "* **SpatialDropout2D** - This version performs the same function as Dropout; however, it drops entire 2D feature maps instead of individual elements\n",
        "* **SpatialDropout3D** - This version performs the same function as Dropout; however, it drops entire 3D feature maps instead of individual elements. \n",
        "\n",
        "There is always trial and error for choosing a good number of neurons and hidden layers. Generally, the number of neurons on each layer will be larger closer to the hidden layer and smaller towards the output layer. This configuration gives the neural network a somewhat triangular or trapezoid appearance.\n",
        "\n",
        "## Activation Functions\n",
        "\n",
        "Activation functions are a choice that you must make for each layer. Generally, you can follow this guideline:\n",
        "* Hidden Layers - RELU\n",
        "* Output Layer - Softmax for classification, linear for regression.\n",
        "\n",
        "Some of the common activation functions in Keras are listed here:\n",
        "\n",
        "* **softmax** - Used for multi-class classification.  Ensures all output neurons behave as probabilities and sum to 1.0.\n",
        "* **elu** - Exponential linear unit.  Exponential Linear Unit or its widely known name ELU is a function that tends to converge cost to zero faster and produce more accurate results. Can produce negative outputs.\n",
        "* **selu** - Scaled Exponential Linear Unit (SELU), essentially **elu** multiplied by a scaling constant.\n",
        "* **softplus** - Softplus activation function. $log(exp(x) + 1)$  [Introduced](https://papers.nips.cc/paper/1920-incorporating-second-order-functional-knowledge-for-better-option-pricing.pdf) in 2001.\n",
        "* **softsign** Softsign activation function. $x / (abs(x) + 1)$ Similar to tanh, but not widely used.\n",
        "* **relu** - Very popular neural network activation function.  Used for hidden layers, cannot output negative values. No trainable parameters.\n",
        "* **tanh** Classic neural network activation function, though often replaced by relu family on modern networks.\n",
        "* **sigmoid** - Classic neural network activation.  Often used on output layer of a binary classifier.\n",
        "* **hard_sigmoid** - Less computationally expensive variant of sigmoid.\n",
        "* **exponential** - Exponential (base e) activation function.\n",
        "* **linear** - Pass-through activation function. Usually used on the output layer of a regression neural network.\n",
        "\n",
        "For more information about Keras activation functions refer to the following:\n",
        "\n",
        "* [Keras Activation Functions](https://keras.io/activations/)\n",
        "* [Activation Function Cheat Sheets](https://ml-cheatsheet.readthedocs.io/en/latest/activation_functions.html)\n",
        "\n",
        "\n",
        "### Advanced Activation Functions\n",
        "\n",
        "Hyperparameters are not changed when the neural network trains. You, the network designer, must define the hyperparameters. The neural network learns regular parameters during neural network training. Neural network weights are the most common type of regular parameter. The \"[advanced activation functions](https://keras.io/layers/advanced-activations/),\" as Keras call them, also contain parameters that the network will learn during training. These activation functions may give you better performance than RELU.\n",
        "\n",
        "* **LeakyReLU** - Leaky version of a Rectified Linear Unit. It allows a small gradient when the unit is not active, controlled by alpha hyperparameter.\n",
        "* **PReLU** - Parametric Rectified Linear Unit, learns the alpha hyperparameter. \n",
        "\n",
        "## Regularization: L1, L2, Dropout\n",
        "\n",
        "\n",
        "* [Keras Regularization](https://keras.io/regularizers/)\n",
        "* [Keras Dropout](https://keras.io/layers/core/)\n",
        "\n",
        "## Batch Normalization\n",
        "\n",
        "* [Keras Batch Normalization](https://keras.io/layers/normalization/)\n",
        "\n",
        "Normalize the activations of the previous layer at each batch, i.e. applies a transformation that maintains the mean activation close to 0 and the activation standard deviation close to 1. Can allow learning rate to be larger.\n",
        "\n",
        "\n",
        "## Training Parameters\n",
        "\n",
        "* [Keras Optimizers](https://keras.io/optimizers/)\n",
        "\n",
        "* **Batch Size** - Usually small, such as 32 or so.\n",
        "* **Learning Rate**  - Usually small, 1e-3 or so.\n"
      ],
      "metadata": {
        "id": "O7HzBX3E5G4j"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SzfTEj7foUiK",
        "outputId": "ba51d7fb-070a-40c9-964d-82718808f88c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Note: using Google CoLab\n"
          ]
        }
      ],
      "source": [
        "# Startup Google CoLab\n",
        "try:\n",
        "    %tensorflow_version 2.x\n",
        "    COLAB = True\n",
        "    print(\"Note: using Google CoLab\")\n",
        "except:\n",
        "    print(\"Note: not using Google CoLab\")\n",
        "    COLAB = False\n",
        "\n",
        "# Nicely formatted time string\n",
        "def hms_string(sec_elapsed):\n",
        "    h = int(sec_elapsed / (60 * 60))\n",
        "    m = int((sec_elapsed % (60 * 60)) / 60)\n",
        "    s = sec_elapsed % 60\n",
        "    return \"{}:{:>02}:{:>05.2f}\".format(h, m, s)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WydBc__qoUiL"
      },
      "source": [
        "# Bayesian Hyperparameter Optimization for Keras\n",
        "\n",
        "Bayesian Hyperparameter Optimization is a method of finding hyperparameters more efficiently than a grid search. Because each candidate set of hyperparameters requires a retraining of the neural network, it is best to keep the number of candidate sets to a minimum. Bayesian Hyperparameter Optimization achieves this by training a model to predict good candidate sets of hyperparameters. [[Cite:snoek2012practical]](https://arxiv.org/pdf/1206.2944.pdf)\n",
        "\n",
        "* [bayesian-optimization](https://github.com/fmfn/BayesianOptimization)\n",
        "* [hyperopt](https://github.com/hyperopt/hyperopt)\n",
        "* [spearmint](https://github.com/JasperSnoek/spearmint)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "NNKv5zWgoUiL"
      },
      "outputs": [],
      "source": [
        "# Ignore useless W0819 warnings generated by TensorFlow 2.0.  \n",
        "# Hopefully can remove this ignore in the future.\n",
        "# See https://github.com/tensorflow/tensorflow/issues/31308\n",
        "import logging, os\n",
        "logging.disable(logging.WARNING)\n",
        "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
        "\n",
        "import pandas as pd\n",
        "from scipy.stats import zscore\n",
        "\n",
        "# Read the data set\n",
        "df = pd.read_csv(\n",
        "    \"https://data.heatonresearch.com/data/t81-558/jh-simple-dataset.csv\",\n",
        "    na_values=['NA','?'])\n",
        "\n",
        "# Generate dummies for job\n",
        "df = pd.concat([df,pd.get_dummies(df['job'],prefix=\"job\")],axis=1)\n",
        "df.drop('job', axis=1, inplace=True)\n",
        "\n",
        "# Generate dummies for area\n",
        "df = pd.concat([df,pd.get_dummies(df['area'],prefix=\"area\")],axis=1)\n",
        "df.drop('area', axis=1, inplace=True)\n",
        "\n",
        "# Missing values for income\n",
        "med = df['income'].median()\n",
        "df['income'] = df['income'].fillna(med)\n",
        "\n",
        "# Standardize ranges\n",
        "df['income'] = zscore(df['income'])\n",
        "df['aspect'] = zscore(df['aspect'])\n",
        "df['save_rate'] = zscore(df['save_rate'])\n",
        "df['age'] = zscore(df['age'])\n",
        "df['subscriptions'] = zscore(df['subscriptions'])\n",
        "\n",
        "# Convert to numpy - Classification\n",
        "x_columns = df.columns.drop('product').drop('id')\n",
        "x = df[x_columns].values\n",
        "dummies = pd.get_dummies(df['product']) # Classification\n",
        "products = dummies.columns\n",
        "y = dummies.values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9qetzyCcoUiM"
      },
      "source": [
        "Now that we've preprocessed the data, we can begin the hyperparameter optimization.  We start by creating a function that generates the model based on just three parameters.  Bayesian optimization works on a vector of numbers, not on a problematic notion like how many layers and neurons are on each layer.  To represent this complex neuron structure as a vector, we use several numbers to describe this structure.   \n",
        "\n",
        "* **dropout** - The dropout percent for each layer.\n",
        "* **neuronPct** - What percent of our fixed 5,000 maximum number of neurons do we wish to use?  This parameter specifies the total count of neurons in the entire network.\n",
        "* **neuronShrink** - Neural networks usually start with more neurons on the first hidden layer and then decrease this count for additional layers.  This percent specifies how much to shrink subsequent layers based on the previous layer.  We stop adding more layers once we run out of neurons (the count specified by neuronPct).\n",
        "\n",
        "These three numbers define the structure of the neural network.  The commends in the below code show exactly how the program constructs the network."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "d6j5HvCqoUiM"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import numpy as np\n",
        "import time\n",
        "import tensorflow.keras.initializers\n",
        "import statistics\n",
        "import tensorflow.keras\n",
        "from sklearn import metrics\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Activation, Dropout, InputLayer\n",
        "from tensorflow.keras import regularizers\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from sklearn.model_selection import StratifiedShuffleSplit\n",
        "from sklearn.model_selection import ShuffleSplit\n",
        "from tensorflow.keras.layers import LeakyReLU,PReLU\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "def generate_model(dropout, neuronPct, neuronShrink):\n",
        "    # We start with some percent of 5000 starting neurons on \n",
        "    # the first hidden layer.\n",
        "    neuronCount = int(neuronPct * 5000)\n",
        "    \n",
        "    # Construct neural network\n",
        "    model = Sequential()\n",
        "\n",
        "    # So long as there would have been at least 25 neurons and \n",
        "    # fewer than 10\n",
        "    # layers, create a new layer.\n",
        "    layer = 0\n",
        "    while neuronCount>25 and layer<10:\n",
        "        # The first (0th) layer needs an input input_dim(neuronCount)\n",
        "        if layer==0:\n",
        "            model.add(Dense(neuronCount, \n",
        "                input_dim=x.shape[1], \n",
        "                activation=PReLU()))\n",
        "        else:\n",
        "            model.add(Dense(neuronCount, activation=PReLU())) \n",
        "        layer += 1\n",
        "\n",
        "        # Add dropout after each hidden layer\n",
        "        model.add(Dropout(dropout))\n",
        "\n",
        "        # Shrink neuron count for each layer\n",
        "        neuronCount = neuronCount * neuronShrink\n",
        "\n",
        "    model.add(Dense(y.shape[1],activation='softmax')) # Output\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LGfXROENoUiN"
      },
      "source": [
        "We can test this code to see how it creates a neural network based on three such parameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nCQC86b_oUiN",
        "outputId": "1ca17af0-895c-40a6-ffe1-99654446c0de"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense (Dense)               (None, 500)               24500     \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 500)               0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 125)               62750     \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 125)               0         \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 31)                3937      \n",
            "                                                                 \n",
            " dropout_2 (Dropout)         (None, 31)                0         \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 7)                 224       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 91,411\n",
            "Trainable params: 91,411\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "# Generate a model and see what the resulting structure looks like.\n",
        "model = generate_model(dropout=0.2, neuronPct=0.1, neuronShrink=0.25)\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vgY6GprqoUiN"
      },
      "source": [
        "We will now create a function to evaluate the neural network using three such parameters.  We use bootstrapping because one training run might have \"bad luck\" with the assigned random weights.  We use this function to train and then evaluate the neural network.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "kbX4k-ynoUiN"
      },
      "outputs": [],
      "source": [
        "SPLITS = 2\n",
        "EPOCHS = 500\n",
        "PATIENCE = 10\n",
        "\n",
        "def evaluate_network(dropout,learning_rate,neuronPct,neuronShrink):\n",
        "    # Bootstrap\n",
        "\n",
        "    # for Classification\n",
        "    boot = StratifiedShuffleSplit(n_splits=SPLITS, test_size=0.1)\n",
        "    # for Regression\n",
        "    # boot = ShuffleSplit(n_splits=SPLITS, test_size=0.1)\n",
        "\n",
        "    # Track progress\n",
        "    mean_benchmark = []\n",
        "    epochs_needed = []\n",
        "    num = 0\n",
        "    \n",
        "    # Loop through samples\n",
        "    for train, test in boot.split(x,df['product']):\n",
        "        start_time = time.time()\n",
        "        num+=1\n",
        "\n",
        "        # Split train and test\n",
        "        x_train = x[train]\n",
        "        y_train = y[train]\n",
        "        x_test = x[test]\n",
        "        y_test = y[test]\n",
        "\n",
        "        model = generate_model(dropout, neuronPct, neuronShrink)\n",
        "        model.compile(loss='categorical_crossentropy', \n",
        "                      optimizer=Adam(learning_rate=learning_rate))\n",
        "        monitor = EarlyStopping(monitor='val_loss', min_delta=1e-3, \n",
        "        patience=PATIENCE, verbose=0, mode='auto', \n",
        "                                restore_best_weights=True)\n",
        "\n",
        "        # Train on the bootstrap sample\n",
        "        model.fit(x_train,y_train,validation_data=(x_test,y_test),\n",
        "                  callbacks=[monitor],verbose=0,epochs=EPOCHS)\n",
        "        epochs = monitor.stopped_epoch\n",
        "        epochs_needed.append(epochs)\n",
        "\n",
        "        # Predict on the out of boot (validation)\n",
        "        pred = model.predict(x_test)\n",
        "\n",
        "        # Measure this bootstrap's log loss\n",
        "        y_compare = np.argmax(y_test,axis=1) # For log loss calculation\n",
        "        score = metrics.log_loss(y_compare, pred)\n",
        "        mean_benchmark.append(score)\n",
        "        m1 = statistics.mean(mean_benchmark)\n",
        "        m2 = statistics.mean(epochs_needed)\n",
        "        mdev = statistics.pstdev(mean_benchmark)\n",
        "\n",
        "        # Record this iteration\n",
        "        time_took = time.time() - start_time\n",
        "        \n",
        "    tensorflow.keras.backend.clear_session()\n",
        "    return (-m1)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZpyYvzKnoUiN",
        "outputId": "5a45e9ac-3c99-445e-f7f2-6e92602b651f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-0.7455484813312068\n"
          ]
        }
      ],
      "source": [
        "print(evaluate_network(\n",
        "    dropout=0.2,\n",
        "    learning_rate=1e-3,\n",
        "    neuronPct=0.2,\n",
        "    neuronShrink=0.2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b05ssrH0qfdz"
      },
      "source": [
        "First, we must install the Bayesian optimization package if we are in Colab."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S2iFisjIqjvO",
        "outputId": "5bee851a-4fc9-47c9-c89d-ca1913468c00"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting bayesian-optimization\n",
            "  Downloading bayesian-optimization-1.2.0.tar.gz (14 kB)\n",
            "Requirement already satisfied: numpy>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from bayesian-optimization) (1.21.6)\n",
            "Requirement already satisfied: scipy>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from bayesian-optimization) (1.4.1)\n",
            "Requirement already satisfied: scikit-learn>=0.18.0 in /usr/local/lib/python3.7/dist-packages (from bayesian-optimization) (1.0.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.18.0->bayesian-optimization) (3.1.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.18.0->bayesian-optimization) (1.1.0)\n",
            "Building wheels for collected packages: bayesian-optimization\n",
            "  Building wheel for bayesian-optimization (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for bayesian-optimization: filename=bayesian_optimization-1.2.0-py3-none-any.whl size=11685 sha256=2312074622448467db0cdf8476cd55dcfa1e47b285165ad78199a0bbabae81bb\n",
            "  Stored in directory: /root/.cache/pip/wheels/fd/9b/71/f127d694e02eb40bcf18c7ae9613b88a6be4470f57a8528c5b\n",
            "Successfully built bayesian-optimization\n",
            "Installing collected packages: bayesian-optimization\n",
            "Successfully installed bayesian-optimization-1.2.0\n"
          ]
        }
      ],
      "source": [
        "# HIDE OUTPUT\n",
        "!pip install bayesian-optimization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vh3nx_SDoUiO"
      },
      "source": [
        "We will now automate this process. We define the bounds for each of these four hyperparameters and begin the Bayesian optimization. Once the program finishes, the best combination of hyperparameters found is displayed. The **optimize** function accepts two parameters that will significantly impact how long the process takes to complete: \n",
        "\n",
        "* **n_iter** - How many steps of Bayesian optimization that you want to perform. The more steps, the more likely you will find a reasonable maximum.\n",
        "* **init_points**: How many steps of random exploration that you want to perform. Random exploration can help by diversifying the exploration space."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FiymzM63oUiO",
        "outputId": "4940746c-5e17-423f-8296-262e79cd7e9a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "|   iter    |  target   |  dropout  | learni... | neuronPct | neuron... |\n",
            "-------------------------------------------------------------------------\n",
            "| \u001b[0m 1       \u001b[0m | \u001b[0m-0.7891  \u001b[0m | \u001b[0m 0.2081  \u001b[0m | \u001b[0m 0.07203 \u001b[0m | \u001b[0m 0.01011 \u001b[0m | \u001b[0m 0.3093  \u001b[0m |\n",
            "| \u001b[95m 2       \u001b[0m | \u001b[95m-0.7768  \u001b[0m | \u001b[95m 0.07323 \u001b[0m | \u001b[95m 0.009234\u001b[0m | \u001b[95m 0.1944  \u001b[0m | \u001b[95m 0.3521  \u001b[0m |\n",
            "| \u001b[0m 3       \u001b[0m | \u001b[0m-21.59   \u001b[0m | \u001b[0m 0.198   \u001b[0m | \u001b[0m 0.05388 \u001b[0m | \u001b[0m 0.425   \u001b[0m | \u001b[0m 0.6884  \u001b[0m |\n",
            "| \u001b[95m 4       \u001b[0m | \u001b[95m-0.7414  \u001b[0m | \u001b[95m 0.102   \u001b[0m | \u001b[95m 0.08781 \u001b[0m | \u001b[95m 0.03711 \u001b[0m | \u001b[95m 0.6738  \u001b[0m |\n",
            "| \u001b[0m 5       \u001b[0m | \u001b[0m-0.9557  \u001b[0m | \u001b[0m 0.2082  \u001b[0m | \u001b[0m 0.05587 \u001b[0m | \u001b[0m 0.149   \u001b[0m | \u001b[0m 0.2061  \u001b[0m |\n",
            "| \u001b[0m 6       \u001b[0m | \u001b[0m-19.86   \u001b[0m | \u001b[0m 0.3996  \u001b[0m | \u001b[0m 0.09683 \u001b[0m | \u001b[0m 0.3203  \u001b[0m | \u001b[0m 0.6954  \u001b[0m |\n",
            "| \u001b[0m 7       \u001b[0m | \u001b[0m-7.02    \u001b[0m | \u001b[0m 0.4373  \u001b[0m | \u001b[0m 0.08946 \u001b[0m | \u001b[0m 0.09419 \u001b[0m | \u001b[0m 0.04866 \u001b[0m |\n",
            "| \u001b[0m 8       \u001b[0m | \u001b[0m-0.7563  \u001b[0m | \u001b[0m 0.08475 \u001b[0m | \u001b[0m 0.08781 \u001b[0m | \u001b[0m 0.1074  \u001b[0m | \u001b[0m 0.4269  \u001b[0m |\n",
            "| \u001b[0m 9       \u001b[0m | \u001b[0m-11.21   \u001b[0m | \u001b[0m 0.478   \u001b[0m | \u001b[0m 0.05332 \u001b[0m | \u001b[0m 0.695   \u001b[0m | \u001b[0m 0.3224  \u001b[0m |\n",
            "| \u001b[0m 10      \u001b[0m | \u001b[0m-1.94    \u001b[0m | \u001b[0m 0.3426  \u001b[0m | \u001b[0m 0.08346 \u001b[0m | \u001b[0m 0.02811 \u001b[0m | \u001b[0m 0.7526  \u001b[0m |\n",
            "| \u001b[0m 11      \u001b[0m | \u001b[0m-3.472   \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 0.01    \u001b[0m | \u001b[0m 0.01    \u001b[0m |\n",
            "| \u001b[0m 12      \u001b[0m | \u001b[0m-18.93   \u001b[0m | \u001b[0m 0.2208  \u001b[0m | \u001b[0m 0.04135 \u001b[0m | \u001b[0m 0.5523  \u001b[0m | \u001b[0m 0.7468  \u001b[0m |\n",
            "| \u001b[0m 13      \u001b[0m | \u001b[0m-1.966   \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 0.01    \u001b[0m | \u001b[0m 1.0     \u001b[0m |\n",
            "| \u001b[0m 14      \u001b[0m | \u001b[0m-14.07   \u001b[0m | \u001b[0m 0.01058 \u001b[0m | \u001b[0m 0.08079 \u001b[0m | \u001b[0m 0.9652  \u001b[0m | \u001b[0m 0.7051  \u001b[0m |\n",
            "| \u001b[0m 15      \u001b[0m | \u001b[0m-6.79    \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 0.1     \u001b[0m | \u001b[0m 0.499   \u001b[0m | \u001b[0m 0.01    \u001b[0m |\n",
            "| \u001b[0m 16      \u001b[0m | \u001b[0m-2.221   \u001b[0m | \u001b[0m 0.499   \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 0.01    \u001b[0m |\n",
            "| \u001b[0m 17      \u001b[0m | \u001b[0m-2.26    \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 0.01    \u001b[0m |\n",
            "| \u001b[0m 18      \u001b[0m | \u001b[0m-19.95   \u001b[0m | \u001b[0m 0.1621  \u001b[0m | \u001b[0m 0.06358 \u001b[0m | \u001b[0m 0.4065  \u001b[0m | \u001b[0m 0.754   \u001b[0m |\n",
            "| \u001b[0m 19      \u001b[0m | \u001b[0m-1.945   \u001b[0m | \u001b[0m 0.499   \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 0.01    \u001b[0m | \u001b[0m 1.0     \u001b[0m |\n",
            "| \u001b[0m 20      \u001b[0m | \u001b[0m-3.3     \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 0.01    \u001b[0m | \u001b[0m 0.3913  \u001b[0m |\n",
            "| \u001b[0m 21      \u001b[0m | \u001b[0m-6.982   \u001b[0m | \u001b[0m 0.004722\u001b[0m | \u001b[0m 0.05502 \u001b[0m | \u001b[0m 0.1704  \u001b[0m | \u001b[0m 0.483   \u001b[0m |\n",
            "| \u001b[0m 22      \u001b[0m | \u001b[0m-4.835   \u001b[0m | \u001b[0m 0.08639 \u001b[0m | \u001b[0m 0.0838  \u001b[0m | \u001b[0m 0.04668 \u001b[0m | \u001b[0m 0.6864  \u001b[0m |\n",
            "| \u001b[0m 23      \u001b[0m | \u001b[0m-17.96   \u001b[0m | \u001b[0m 0.1168  \u001b[0m | \u001b[0m 0.0447  \u001b[0m | \u001b[0m 0.5546  \u001b[0m | \u001b[0m 0.9497  \u001b[0m |\n",
            "| \u001b[0m 24      \u001b[0m | \u001b[0m-0.7583  \u001b[0m | \u001b[0m 0.2148  \u001b[0m | \u001b[0m 0.1     \u001b[0m | \u001b[0m 0.01    \u001b[0m | \u001b[0m 0.5694  \u001b[0m |\n",
            "| \u001b[0m 25      \u001b[0m | \u001b[0m-0.7658  \u001b[0m | \u001b[0m 0.1311  \u001b[0m | \u001b[0m 0.002183\u001b[0m | \u001b[0m 0.1758  \u001b[0m | \u001b[0m 0.6452  \u001b[0m |\n",
            "| \u001b[0m 26      \u001b[0m | \u001b[0m-0.848   \u001b[0m | \u001b[0m 0.215   \u001b[0m | \u001b[0m 0.02843 \u001b[0m | \u001b[0m 0.02005 \u001b[0m | \u001b[0m 0.6937  \u001b[0m |\n",
            "| \u001b[0m 27      \u001b[0m | \u001b[0m-0.7694  \u001b[0m | \u001b[0m 0.04871 \u001b[0m | \u001b[0m 0.1     \u001b[0m | \u001b[0m 0.0823  \u001b[0m | \u001b[0m 0.2258  \u001b[0m |\n",
            "| \u001b[0m 28      \u001b[0m | \u001b[0m-13.58   \u001b[0m | \u001b[0m 0.06672 \u001b[0m | \u001b[0m 0.05078 \u001b[0m | \u001b[0m 0.8902  \u001b[0m | \u001b[0m 0.5715  \u001b[0m |\n",
            "| \u001b[0m 29      \u001b[0m | \u001b[0m-3.589   \u001b[0m | \u001b[0m 0.1184  \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 0.01    \u001b[0m | \u001b[0m 0.563   \u001b[0m |\n",
            "| \u001b[95m 30      \u001b[0m | \u001b[95m-0.7167  \u001b[0m | \u001b[95m 0.4082  \u001b[0m | \u001b[95m 0.01635 \u001b[0m | \u001b[95m 0.02488 \u001b[0m | \u001b[95m 0.1694  \u001b[0m |\n",
            "=========================================================================\n",
            "Total runtime: 2:32:09.33\n",
            "{'target': -0.7167064327385015, 'params': {'dropout': 0.40820293255682355, 'learning_rate': 0.016352671207695214, 'neuronPct': 0.02488288532396405, 'neuronShrink': 0.16937882150810435}}\n"
          ]
        }
      ],
      "source": [
        "from bayes_opt import BayesianOptimization\n",
        "import time\n",
        "\n",
        "# Supress NaN warnings\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\",category =RuntimeWarning)\n",
        "\n",
        "# Bounded region of parameter space\n",
        "pbounds = {'dropout': (0.0, 0.499),\n",
        "           'learning_rate': (0.0, 0.1),\n",
        "           'neuronPct': (0.01, 1),\n",
        "           'neuronShrink': (0.01, 1)\n",
        "          }\n",
        "\n",
        "optimizer = BayesianOptimization(\n",
        "    f=evaluate_network,\n",
        "    pbounds=pbounds,\n",
        "    verbose=2,  # verbose = 1 prints only when a maximum \n",
        "    # is observed, verbose = 0 is silent\n",
        "    random_state=1,\n",
        ")\n",
        "\n",
        "start_time = time.time()\n",
        "optimizer.maximize(init_points=10, n_iter=20,)\n",
        "time_took = time.time() - start_time\n",
        "\n",
        "print(f\"Total runtime: {hms_string(time_took)}\")\n",
        "print(optimizer.max)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tZ7xW5UDqaOV"
      },
      "source": [
        "As you can see, the algorithm performed 30 total iterations. This total iteration count includes ten random and 20 optimization iterations. "
      ]
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "colab": {
      "collapsed_sections": [],
      "name": "test.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.9 (tensorflow)",
      "language": "python",
      "name": "tensorflow"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}